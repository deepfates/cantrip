---
title: "Minh Pham on X: \"Why Most Agent Harnesses Are Not Bitter Lesson Pilled\" / X"
url: "https://x.com/buckeyevn/status/2014171253045960803?rw_tt_thread=True"
date_fetched: "2026-02-16"
type: twitter
---

Agent harnesses are having their moment in 2026. Everyone is busy building sandboxes, wiring up subagents, defining agent skills, and shipping ever-larger agent graphs.

And a lot of it is directionally useful.

But if you squint, most of today's agent harnesses share a common blind spot: they haven't internalized the most important lesson from seven decades of AI research. They're optimized for making the current generation of models usable, not for compounding as models get stronger.

In 2019, Rich Sutton crystallized a pattern that had been hiding in plain sight across AI history. His essay "The Bitter Lesson" observes that general methods leveraging computation consistently outperform approaches based on handcrafted human knowledge by a large margin.

The lesson is "bitter" because it runs counter to researcher intuition. When building chess engines, speech recognizers, or game-playing agents, the natural instinct is to encode domain expertise: rules about pawn structures, phoneme patterns, or strategic principles. This approach yields quick wins and feels intellectually satisfying. But history repeatedly shows that these handcrafted solutions plateau, while methods that scale with compute (e.g. search and learning) eventually dominate.

Consider chess: Deep Blue combined massive search capability with substantial chess knowledge. IBM hired grandmasters and built extensive opening databases. It worked, but the approach required constant human refinement. Then AlphaZero arrived and learned chess from scratch through pure self-play, crushing the best traditional engines. The pattern repeated in Go: AlphaGo initially used human game data, but AlphaGo Zero proved that starting from nothing produced a stronger player.

Sutton's key point is not that domain knowledge is useless, but that the bottleneck is almost always the wrong one. The world is irreducibly complex, and trying to bake our own simplifications into the system does not age well. Instead, you build meta-methods that can find the complexity via scalable computation.

In 2026 terms: if your "agent harness" primarily scales by adding more human-authored structure, it is probably fighting the Bitter Lesson.

A modern agent system has two places where "intelligence" can live:

*   In the model—capabilities learned via large-scale pre-training and post-training RL.

*   In the harness—the scaffolding: workflows, routers, subagents, tools, code executions, handoffs.

Most agent stacks today are doing something like: "models aren't reliable enough, so we will encode reliability in the harness." That is a rational product move. But it is often a Bitter Lesson move in reverse: shifting complexity away from the part that scales (the model) into the part that doesn't (bespoke scaffolding).

Anthropic explicitly warns about this trade: frameworks make it easy to add abstraction layers that obscure prompts and responses and tempt unnecessary complexity, recommending that developers start simple and understand what's happening under the hood.

A caveat before we proceed: the bitter lesson describes long-run tendencies, not immediate prescriptions. The patterns below often work well for current applications. The question is whether they'll continue to work as compute scales.

The Workflow Trap

Anthropic's engineering team draws a crucial distinction between workflows and agents. Workflows are systems where LLMs and tools follow predefined code paths. Agents, by contrast, dynamically direct their own processes and tool usage.

The proliferation of visual workflow builders (including OpenAI's Agent Builder) which offers drag-and-drop nodes for composing logic represents a step away from the bitter lesson. These tools encode human assumptions about task decomposition directly into the system architecture. When you draw a flowchart connecting "research" to "summarize" to "draft," you're embedding your mental model of how the task should proceed.

The trap is when "workflow" becomes the default answer to model limitations: if you need reliability, you add more nodes; if you need coverage, you add more branches; if you need quality, you add more evaluator steps. This scales headcount, not capability.

It also tends to ossify around the current model's failure modes. When the model improves, your workflow doesn't get meaningfully simpler, because the team has already committed to the graph. Worse, the graph can become the primary source of brittleness.

Anthropic is remarkably candid about this tradeoff: workflows offer predictability and consistency for well-defined tasks, but agents are better when flexibility and model-driven decision-making are needed at scale. The key phrase is "at scale", workflows hit a ceiling that agent autonomy does not. From a Bitter Lesson lens, workflows should be the thinnest layer that buys you product guarantees, not the place you keep "encoding intelligence."

The Specialized Subagent Illusion

Perhaps the most seductive anti-pattern is designing systems with specialized subagents: a Researcher agent, a Coder agent, a Writer agent, each with distinct prompts and tool access. This approach mirrors how human organizations function: specialized roles coordinating on complex tasks.

We explored this approach extensively at Invisible, drawing explicit inspiration from human team structures. Our system orchestrates "a hierarchy of specialized agents" that together solve problems requiring human intelligence. It works—often quite well. But the human-inspired architecture encodes assumptions about how to decompose problems that may not generalize as models improve.

The bitter lesson suggests caution here. Human organizational structures evolved under constraints—bounded cognition, communication costs, specialization advantages—that don't necessarily apply to AI systems. When we design agent hierarchies that mirror management structures, we may be importing human limitations rather than leveraging computational abundance.

Even Invisible's own analysis points to the key nuance: the web is dynamic, and a "pre-defined execution plan can't predict, and therefore can't handle, all these variations." The solution we argue for is not "more fixed roles," but dynamic hierarchy: agents delegating to specialized agents at runtime as problems arise. That distinction is everything.

The Bitter Lesson isn't "never use specialists." It's "don't freeze your guess of the right specialists into the architecture."

The For-Loop Ceiling

A countermovement has emerged arguing that fancy frameworks are unnecessary: an LLM in a while loop with good tool design is all you really need. Proponents observe that most production agents boil down to straightforward "LLM + loop + tools" patterns without elaborate orchestration.

On the surface, this seems to embrace the bitter lesson by stripping away elaborate scaffolding. But "loop-only" becomes its own anti-pattern when you treat it as a scaling strategy.

Why? Because your only real knob is number of iterations. You can think longer, call more tools, reflect again. That helps, but it is a one-dimensional scaling axis. It is hard to parallelize, hard to structure, and easy to waste tokens. As tasks get more complex, you need more than "just try again." You need better decomposition, better memory interaction, better retrieval over massive contexts, and better ways to allocate compute.

A loop is a mechanism. It is not a plan for how you'll scale inference-time compute intelligently.

If workflows, specialized subagents, and simple loops all have scaling limitations, what approaches better embody the bitter lesson? Here's the pattern that feels most aligned: methods that turn extra compute into better decisions without relying on a fixed human-designed decomposition. Two approaches stand out.

Dynamic Subagent Spawning

The core move is: don't predefine the team. Let the system create the team as needed.

Anthropic's multi-agent research system demonstrates this powerfully. When a query arrives, a lead agent analyzes it, develops a strategy, and spawns subagents to explore different aspects simultaneously. The system adapts dynamically, spawning new agents as needed and consolidating results as subtasks complete. Critically, subagents act as intelligent filters, iteratively gathering information rather than executing predetermined steps.

LangChain's Deep Agents framework represents this same architectural shift. These systems don't just react in a loop—they combine agentic patterns to plan, manage persistent state, and delegate work to sub-agents that are created on demand. The orchestrator maintains global state tracking overall progress, enabling dynamic reallocation of resources when certain paths prove more fruitful.

From a Bitter Lesson lens, dynamic subagents have two nice properties. First, they push complexity into inference-time search. The agent is effectively searching over decompositions, not following a human-authored blueprint. Second, they age better. As models improve, the delegation policy can improve "for free," without you rewriting the org chart.

This matters for scalability because the branching factor becomes dynamic. Instead of one dimension (iterations), you gain additional scaling dimensions: parallel subagent count, subagent depth, and compute allocation per branch. As models improve at spawning and coordinating subagents, the system can leverage more computation without human redesign.

To be clear, dynamic subagent systems still encode assumptions—tool access, coordination protocols, spawning heuristics. But they encode less than fixed hierarchies, leaving more room for the model to determine optimal task decomposition.

Recursive Language Models: Toward the Bitter Lesson

The most ambitious development is Recursive Language Models (RLMs), introduced by researchers at MIT (Alex Zhang, Tim Kraska, and Omar Khattab) and being productized by organizations like Prime Intellect, who predict this approach will be "the paradigm of 2026."

RLMs represent a radical rethinking of how language models interact with long contexts. Instead of ingesting all tokens directly into the transformer's attention window, RLMs treat the entire prompt as an external string that the model can inspect and transform through code. The context is offloaded into a Python REPL environment, enabling the model to reason over its context through code and recursive self-calls.

The scaling implications are significant. RLMs can handle inputs 100x larger than a model's native attention window: entire codebases, multi-year document archives, book-length texts. On benchmarks involving 6–11 million tokens, RLMs achieved over 91% accuracy where base models and summary agents failed catastrophically. And they can be up to 3x cheaper because the model selectively views context rather than processing everything at once.

This is Bitter Lesson aligned because it treats intelligence as a scalable computation strategy(recursion and decomposition under model control), not as a handcrafted workflow. It creates multiple scaling axes beyond "more turns": recursion depth, subproblem granularity, selective reading and indexing over huge context, adaptive compute allocation.

As the original researchers note, scaling attention and context folding are dual problems—both trying to determine what to forget when examining the past. RLMs don't encode human intuitions about what's important in context. Instead, they let the model learn what to examine, compress, and recurse on. The approach scales with both model capability and context length improvements, allowing each advance in foundation models to translate more directly into better agent performance.

That said, RLMs are early-stage research. The REPL scaffolding is itself a form of structure, and production deployment remains limited. The promise is real, but so is the gap between research benchmarks and deployed systems.

A Practical Rubric

Here's a quick test you can apply to any agent architecture:

If model capability doubles next year, does your system get dramatically simpler, cheaper, or more reliable without major refactors?

If yes, you are probably leaning into the Bitter Lesson. If no, you probably built a lot of "human understanding" into the harness.

More pointedly: if your main scaling plan is "add more nodes/roles," you're scaling humans. If your main scaling plan is "let the model allocate more compute through general methods," you're scaling compute.

The bitter lesson doesn't mean all structure is harmful, it means structure should emerge from learning rather than be imposed through design. The difference between a workflow and a dynamic agent isn't that one has no structure; it's that one's structure is fixed by human assumptions while the other's structure emerges from the task.

In the near term, workflows will keep winning because product needs determinism, auditing, safety controls, and debuggability. Anthropic and OpenAI are both steering builders toward patterns that ship. Anthropic's counsel to start simple is sound.

But over the next couple years, the "agent harness" that wins will look less like a handcrafted org chart and more like a compute allocation engine: dynamic delegation, recursive decomposition, learned control policies, and evaluation loops that are increasingly model-driven rather than rule-driven.

When complexity is warranted, consider forms that scale: dynamic spawning over fixed hierarchies, recursive self-modification over predetermined workflows, learned resource allocation over hardcoded iteration limits.

That's what it means to be Bitter Lesson pilled in agent land: you still build harnesses, but you treat them as thin interfaces to scalable computation, not as the place you stash the intelligence.

History is not kind to clever designs that don't scale. But it's remarkably generous to those who bet on computation.

Thank you to

(MIT, RLM author) and

(LangChain) for reviewing this paper and providing valuable feedback.

Sutton, R. (2019). “The Bitter Lesson.”

Anthropic. “Building Effective Agents.”

Invisible. “Human-Inspired Agent Design in Web Automation.”

Zhang, A., Kraska, T., & Khattab, O. “Recursive Language Models.” MIT.

LangChain. “Deep Agents.”

Prime Intellect. “RLM: The Paradigm of 2026.”

Anthropic. “Building a Multi-Agent Research System.”
