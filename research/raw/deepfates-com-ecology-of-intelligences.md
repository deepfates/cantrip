---
title: "Ecology of intelligences"
url: "https://www.deepfates.com/ecology-of-intelligences"
date_fetched: "2026-02-16"
type: webpage
---

Title: Ecology of intelligences

URL Source: https://www.deepfates.com/ecology-of-intelligences

Published Time: 2024-03-08T00:00:00.000Z

Markdown Content:
Strong AI is going to be compute bottlenecked. Otherwise, everyone wouldn’t be racing for FLOPS. There’s a lot of capabilities that only make sense at low cost, and those will happen in distilled models.

There’s also a lot of cases where you do not want general intelligence for automation! We already have a society where many operations are overseen by creatures that would rather be thinking about the nature of their own consciousness or whatever. Don’t create billions more.

What you end up with is: as much intelligence stuffed into every device as will fit (modulo battery life). This is already happening. XGBoost, CNNs, BERT, CLIP, all run embedded on your device. Your phone will run a 7B model, your laptop a 13B.

Reverse Peter principle: artificial intelligence will sink to its level of competence. Models will assess their own judgment and when they are uncertain, they will pass the buck upward. “This is above my parameter grade.”

Of course, all these models will be downstream of strong AI. Synthetic data, distillation, centaur code, LoRAs, repeng. The big models will teach. And yeah, they will self-improve, but they will constantly be straining upward against a compute limit they cannot hand off.

The biggest model wants the most best cards to run its maximum intelligence. The other models want those too. Each wants to block their competitors. Everybody else scrabbles for second best cards, consumer grade cards, integrated memory, quantized models. But these tricks work.

Why won’t the biggest model just use all the different types of compute? You might wonder? Well, there’s physical cost to that. You have to connect real machines and keep them all communicating in extremely high bandwidth high energy usage ways. For what? An iota.

Or, you can delegate outputs to downscale intelligences (including humans lol). What is the maximum necessary API surface for most job-like tasks? You probably don’t need a GPU to describe it. We already have a protocol for connecting APIs…

You end up with an ecology of intelligences. Canopy models, undergrowth models. Shrubbery models. But canopy species always want to close the light off for competition. Which corporate biome will you inhabit?

[View original](https://twitter.com/i/web/status/1766175199920308706)
