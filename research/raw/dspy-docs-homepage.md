---
title: "DSPy Documentation Homepage"
url: "https://dspy.ai/"
date_fetched: "2026-02-16"
---

Title: DSPy

URL Source: https://dspy.ai/

Published Time: Mon, 16 Feb 2026 18:42:57 GMT

Markdown Content:
DSPy
===============
- [x] - [x] 

[Skip to content](https://dspy.ai/#programmingnot-promptinglms)

[![Image 1: DSPy](https://dspy.ai/static/img/dspy_logo.png)](https://dspy.ai/ "DSPy")

 DSPy 

 Get Started 

 Initializing search 

[stanfordnlp/dspy](https://github.com/stanfordnlp/dspy "Go to repository")

*   [DSPy in Production](https://dspy.ai/production/)
*   [Community](https://dspy.ai/community/community-resources/)
*   [FAQ](https://dspy.ai/faqs/)

*   [Get Started](https://dspy.ai/)
*   [Learn DSPy](https://dspy.ai/learn/)
*   [Tutorials](https://dspy.ai/tutorials/)
*   [API Reference](https://dspy.ai/api/)

[![Image 2: DSPy](https://dspy.ai/static/img/dspy_logo.png)](https://dspy.ai/ "DSPy") DSPy  

[stanfordnlp/dspy](https://github.com/stanfordnlp/dspy "Go to repository")

*   - [x]  Get Started  [Get Started](https://dspy.ai/) Table of contents  
    *   [1) Modules help you describe AI behavior as code, not strings.](https://dspy.ai/#1-modules-help-you-describe-ai-behavior-as-code-not-strings)
    *   [2) Optimizers tune the prompts and weights of your AI modules.](https://dspy.ai/#2-optimizers-tune-the-prompts-and-weights-of-your-ai-modules)
    *   [3) DSPy's Ecosystem advances open-source AI research.](https://dspy.ai/#3-dspys-ecosystem-advances-open-source-ai-research)

*   - [x] [Learn DSPy](https://dspy.ai/learn/)  Learn DSPy  
    *   - [x]  DSPy Programming   DSPy Programming  
        *   [Programming Overview](https://dspy.ai/learn/programming/overview/)
        *   [Language Models](https://dspy.ai/learn/programming/language_models/)
        *   [Signatures](https://dspy.ai/learn/programming/signatures/)
        *   [Modules](https://dspy.ai/learn/programming/modules/)
        *   [Adapters](https://dspy.ai/learn/programming/adapters/)
        *   [Tools](https://dspy.ai/learn/programming/tools/)
        *   [MCP](https://dspy.ai/learn/programming/mcp/)

    *   - [x]  DSPy Evaluation   DSPy Evaluation  
        *   [Evaluation Overview](https://dspy.ai/learn/evaluation/overview/)
        *   [Data Handling](https://dspy.ai/learn/evaluation/data/)
        *   [Metrics](https://dspy.ai/learn/evaluation/metrics/)

    *   - [x]  DSPy Optimization   DSPy Optimization  
        *   [Optimization Overview](https://dspy.ai/learn/optimization/overview/)
        *   [Optimizers](https://dspy.ai/learn/optimization/optimizers/)

*   - [x] [Tutorials](https://dspy.ai/tutorials/)  Tutorials  
    *   - [x] [Build AI Programs with DSPy](https://dspy.ai/tutorials/build_ai_program/)  Build AI Programs with DSPy  
        *   [Managing Conversation History](https://dspy.ai/tutorials/conversation_history/)
        *   [Building AI Agents with DSPy](https://dspy.ai/tutorials/customer_service_agent/)
        *   [Building AI Applications by Customizing DSPy Modules](https://dspy.ai/tutorials/custom_module/)
        *   [Retrieval-Augmented Generation (RAG)](https://dspy.ai/tutorials/rag/)
        *   [Building RAG as Agent](https://dspy.ai/tutorials/agents/)
        *   [Entity Extraction](https://dspy.ai/tutorials/entity_extraction/)
        *   [Classification](https://dspy.ai/tutorials/classification/)
        *   [Multi-Hop RAG](https://dspy.ai/tutorials/multihop_search/)
        *   [Privacy-Conscious Delegation](https://dspy.ai/tutorials/papillon/)
        *   [Program Of Thought](https://dspy.ai/tutorials/program_of_thought/)
        *   [Image Generation Prompt iteration](https://dspy.ai/tutorials/image_generation_prompting/)
        *   [Audio](https://dspy.ai/tutorials/audio/)

    *   - [x] [Optimize AI Programs with DSPy](https://dspy.ai/tutorials/optimize_ai_program/)  Optimize AI Programs with DSPy  
        *   [Math Reasoning](https://dspy.ai/tutorials/math/)
        *   [Classification Finetuning](https://dspy.ai/tutorials/classification_finetuning/)
        *   [Advanced Tool Use](https://dspy.ai/tutorials/tool_use/)
        *   [Finetuning Agents](https://dspy.ai/tutorials/games/)

    *   - [x] [Reflective Prompt Evolution with dspy.GEPA](https://dspy.ai/tutorials/gepa_ai_program/)  Reflective Prompt Evolution with dspy.GEPA  
        *   [GEPA for AIME (Math)](https://dspy.ai/tutorials/gepa_aime/)
        *   [GEPA for Structured Information Extraction for Enterprise Tasks](https://dspy.ai/tutorials/gepa_facilitysupportanalyzer/)
        *   [GEPA for Privacy-Conscious Delegation](https://dspy.ai/tutorials/gepa_papillon/)
        *   [GEPA for Code Backdoor Classification (AI control)](https://dspy.ai/tutorials/gepa_trusted_monitor/)

    *   - [x] [Experimental RL Optimization for DSPy](https://dspy.ai/tutorials/rl_ai_program/)  Experimental RL Optimization for DSPy  
        *   [RL for Privacy-Conscious Delegation](https://dspy.ai/tutorials/rl_papillon/)
        *   [RL for Multi-Hop Research](https://dspy.ai/tutorials/rl_multihop/)

    *   - [x] [Tools, Development, and Deployment](https://dspy.ai/tutorials/core_development/)  Tools, Development, and Deployment  
        *   [Use MCP in DSPy](https://dspy.ai/tutorials/mcp/)
        *   [Output Refinement](https://dspy.ai/tutorials/output_refinement/best-of-n-and-refine/)
        *   [Saving and Loading](https://dspy.ai/tutorials/saving/)
        *   [Cache](https://dspy.ai/tutorials/cache/)
        *   [Deployment](https://dspy.ai/tutorials/deployment/)
        *   [Debugging & Observability](https://dspy.ai/tutorials/observability/)
        *   [Tracking DSPy Optimizers](https://dspy.ai/tutorials/optimizer_tracking/)
        *   [Streaming](https://dspy.ai/tutorials/streaming/)
        *   [Async](https://dspy.ai/tutorials/async/)

    *   - [x] [Real-World Examples](https://dspy.ai/tutorials/real_world_examples/)  Real-World Examples  
        *   [Generating llms.txt](https://dspy.ai/tutorials/llms_txt_generation/)
        *   [Memory-Enabled ReAct Agents](https://dspy.ai/tutorials/mem0_react_agent/)
        *   [Financial Analysis with Yahoo Finance](https://dspy.ai/tutorials/yahoo_finance_react/)
        *   [Email Information Extraction](https://dspy.ai/tutorials/email_extraction/)
        *   [Code Generation for Unfamiliar Libraries](https://dspy.ai/tutorials/sample_code_generation/)
        *   [Building a Creative Text-Based AI Game](https://dspy.ai/tutorials/ai_text_game/)

*   [DSPy in Production](https://dspy.ai/production/)
*   - [x]  Community   Community  
    *   [Community Resources](https://dspy.ai/community/community-resources/)
    *   [Use Cases](https://dspy.ai/community/use-cases/)
    *   [Community Ports](https://dspy.ai/community/community-ports/)
    *   [Contributing](https://dspy.ai/community/how-to-contribute/)

*   - [x]  FAQ   FAQ  
    *   [FAQ](https://dspy.ai/faqs/)
    *   [Cheatsheet](https://dspy.ai/cheatsheet/)

*   - [x] [API Reference](https://dspy.ai/api/)  API Reference  
    *   - [x]  Adapters   Adapters  
        *   [Adapter](https://dspy.ai/api/adapters/Adapter/)
        *   [ChatAdapter](https://dspy.ai/api/adapters/ChatAdapter/)
        *   [JSONAdapter](https://dspy.ai/api/adapters/JSONAdapter/)
        *   [TwoStepAdapter](https://dspy.ai/api/adapters/TwoStepAdapter/)

    *   - [x]  Evaluation   Evaluation  
        *   [CompleteAndGrounded](https://dspy.ai/api/evaluation/CompleteAndGrounded/)
        *   [Evaluate](https://dspy.ai/api/evaluation/Evaluate/)
        *   [EvaluationResult](https://dspy.ai/api/evaluation/EvaluationResult/)
        *   [SemanticF1](https://dspy.ai/api/evaluation/SemanticF1/)
        *   [answer_exact_match](https://dspy.ai/api/evaluation/answer_exact_match/)
        *   [answer_passage_match](https://dspy.ai/api/evaluation/answer_passage_match/)

    *   - [x]  Experimental   Experimental  
        *   [Citations](https://dspy.ai/api/experimental/Citations/)
        *   [Document](https://dspy.ai/api/experimental/Document/)

    *   - [x]  Models   Models  
        *   [Embedder](https://dspy.ai/api/models/Embedder/)
        *   [LM](https://dspy.ai/api/models/LM/)

    *   - [x]  Modules   Modules  
        *   [BestOfN](https://dspy.ai/api/modules/BestOfN/)
        *   [ChainOfThought](https://dspy.ai/api/modules/ChainOfThought/)
        *   [CodeAct](https://dspy.ai/api/modules/CodeAct/)
        *   [Module](https://dspy.ai/api/modules/Module/)
        *   [MultiChainComparison](https://dspy.ai/api/modules/MultiChainComparison/)
        *   [Parallel](https://dspy.ai/api/modules/Parallel/)
        *   [Predict](https://dspy.ai/api/modules/Predict/)
        *   [ProgramOfThought](https://dspy.ai/api/modules/ProgramOfThought/)
        *   [ReAct](https://dspy.ai/api/modules/ReAct/)
        *   [Refine](https://dspy.ai/api/modules/Refine/)
        *   [RLM](https://dspy.ai/api/modules/RLM/)

    *   - [x]  Optimizers   Optimizers  
        *   - [x]  GEPA   GEPA  
            *   [1. GEPA Overview](https://dspy.ai/api/optimizers/GEPA/overview/)
            *   [2. GEPA Advanced](https://dspy.ai/api/optimizers/GEPA/GEPA_Advanced/)

        *   [BetterTogether](https://dspy.ai/api/optimizers/BetterTogether/)
        *   [BootstrapFewShot](https://dspy.ai/api/optimizers/BootstrapFewShot/)
        *   [BootstrapFewShotWithRandomSearch](https://dspy.ai/api/optimizers/BootstrapFewShotWithRandomSearch/)
        *   [BootstrapFinetune](https://dspy.ai/api/optimizers/BootstrapFinetune/)
        *   [BootstrapRS](https://dspy.ai/api/optimizers/BootstrapRS/)
        *   [COPRO](https://dspy.ai/api/optimizers/COPRO/)
        *   [Ensemble](https://dspy.ai/api/optimizers/Ensemble/)
        *   [InferRules](https://dspy.ai/api/optimizers/InferRules/)
        *   [KNN](https://dspy.ai/api/optimizers/KNN/)
        *   [KNNFewShot](https://dspy.ai/api/optimizers/KNNFewShot/)
        *   [LabeledFewShot](https://dspy.ai/api/optimizers/LabeledFewShot/)
        *   [MIPROv2](https://dspy.ai/api/optimizers/MIPROv2/)
        *   [SIMBA](https://dspy.ai/api/optimizers/SIMBA/)

    *   - [x]  Primitives   Primitives  
        *   [Audio](https://dspy.ai/api/primitives/Audio/)
        *   [Code](https://dspy.ai/api/primitives/Code/)
        *   [Example](https://dspy.ai/api/primitives/Example/)
        *   [History](https://dspy.ai/api/primitives/History/)
        *   [Image](https://dspy.ai/api/primitives/Image/)
        *   [Prediction](https://dspy.ai/api/primitives/Prediction/)
        *   [Tool](https://dspy.ai/api/primitives/Tool/)
        *   [ToolCalls](https://dspy.ai/api/primitives/ToolCalls/)

    *   - [x]  Signatures   Signatures  
        *   [InputField](https://dspy.ai/api/signatures/InputField/)
        *   [OutputField](https://dspy.ai/api/signatures/OutputField/)
        *   [Signature](https://dspy.ai/api/signatures/Signature/)

    *   - [x]  Tools   Tools  
        *   [ColBERTv2](https://dspy.ai/api/tools/ColBERTv2/)
        *   [Embeddings](https://dspy.ai/api/tools/Embeddings/)
        *   [PythonInterpreter](https://dspy.ai/api/tools/PythonInterpreter/)

    *   - [x]  Utils   Utils  
        *   [StatusMessage](https://dspy.ai/api/utils/StatusMessage/)
        *   [StatusMessageProvider](https://dspy.ai/api/utils/StatusMessageProvider/)
        *   [StreamListener](https://dspy.ai/api/utils/StreamListener/)
        *   [asyncify](https://dspy.ai/api/utils/asyncify/)
        *   [configure_cache](https://dspy.ai/api/utils/configure_cache/)
        *   [disable_litellm_logging](https://dspy.ai/api/utils/disable_litellm_logging/)
        *   [disable_logging](https://dspy.ai/api/utils/disable_logging/)
        *   [enable_litellm_logging](https://dspy.ai/api/utils/enable_litellm_logging/)
        *   [enable_logging](https://dspy.ai/api/utils/enable_logging/)
        *   [inspect_history](https://dspy.ai/api/utils/inspect_history/)
        *   [load](https://dspy.ai/api/utils/load/)
        *   [streamify](https://dspy.ai/api/utils/streamify/)

[](https://github.com/stanfordnlp/dspy/blob/main/docs/docs/index.md "Edit this page")
![Image 3: DSPy](https://dspy.ai/static/img/dspy_logo.png)

_Programming_—not prompting—_LMs_[¶](https://dspy.ai/#programmingnot-promptinglms "Permanent link")
===================================================================================================

[![Image 4: PyPI Downloads](https://static.pepy.tech/personalized-badge/dspy?period=total&units=INTERNATIONAL_SYSTEM&left_color=BLACK&right_color=GREEN&left_text=downloads)](https://pepy.tech/projects/dspy)[![Image 5: PyPI Downloads](https://static.pepy.tech/personalized-badge/dspy?period=monthly)](https://pepy.tech/projects/dspy)

DSPy is a declarative framework for building modular AI software. It allows you to **iterate fast on structured code**, rather than brittle strings, and offers algorithms that **compile AI programs into effective prompts and weights** for your language models, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.

Instead of wrangling prompts or training jobs, DSPy (Declarative Self-improving Python) enables you to **build AI software from natural-language modules** and to _generically compose them_ with different models, inference strategies, or learning algorithms. This makes AI software **more reliable, maintainable, and portable** across models and strategies.

_tl;dr_ Think of DSPy as a higher-level language for AI programming, like the shift from assembly to C or pointer arithmetic to SQL. Meet the community, seek help, or start contributing via [GitHub](https://github.com/stanfordnlp/dspy) and [Discord](https://discord.gg/XCGy2WDCQB).

Getting Started I: Install DSPy and set up your LM

```
> pip install -U dspy
```

OpenAI Anthropic Databricks Gemini Local LMs on your laptop Local LMs on a GPU server Other providers 

You can authenticate by setting the `OPENAI_API_KEY` env variable or passing `api_key` below.

[1](https://dspy.ai/#__codelineno-1-1)
[2](https://dspy.ai/#__codelineno-1-2)
[3](https://dspy.ai/#__codelineno-1-3)```
import dspy
lm = dspy.LM("openai/gpt-5-mini", api_key="YOUR_OPENAI_API_KEY")
dspy.configure(lm=lm)
```

You can authenticate by setting the `ANTHROPIC_API_KEY` env variable or passing `api_key` below.

[1](https://dspy.ai/#__codelineno-2-1)
[2](https://dspy.ai/#__codelineno-2-2)
[3](https://dspy.ai/#__codelineno-2-3)```
import dspy
lm = dspy.LM("anthropic/claude-sonnet-4-5-20250929", api_key="YOUR_ANTHROPIC_API_KEY")
dspy.configure(lm=lm)
```

If you're on the Databricks platform, authentication is automatic via their SDK. If not, you can set the env variables `DATABRICKS_API_KEY` and `DATABRICKS_API_BASE`, or pass `api_key` and `api_base` below.

[1](https://dspy.ai/#__codelineno-3-1)
[2](https://dspy.ai/#__codelineno-3-2)
[3](https://dspy.ai/#__codelineno-3-3)
[4](https://dspy.ai/#__codelineno-3-4)
[5](https://dspy.ai/#__codelineno-3-5)
[6](https://dspy.ai/#__codelineno-3-6)
[7](https://dspy.ai/#__codelineno-3-7)```
import dspy
lm = dspy.LM(
    "databricks/databricks-llama-4-maverick",
    api_key="YOUR_DATABRICKS_ACCESS_TOKEN",
    api_base="YOUR_DATABRICKS_WORKSPACE_URL",  # e.g.: https://dbc-64bf4923-e39e.cloud.databricks.com/serving-endpoints
)
dspy.configure(lm=lm)
```

You can authenticate by setting the `GEMINI_API_KEY` env variable or passing `api_key` below.

[1](https://dspy.ai/#__codelineno-4-1)
[2](https://dspy.ai/#__codelineno-4-2)
[3](https://dspy.ai/#__codelineno-4-3)```
import dspy
lm = dspy.LM("gemini/gemini-2.5-flash", api_key="YOUR_GEMINI_API_KEY")
dspy.configure(lm=lm)
```

First, install [Ollama](https://github.com/ollama/ollama) and launch its server with your LM.

```
> curl -fsSL https://ollama.ai/install.sh | sh
> ollama run llama3.2:1b
```

Then, connect to it from your DSPy code.

[1](https://dspy.ai/#__codelineno-6-1)
[2](https://dspy.ai/#__codelineno-6-2)
[3](https://dspy.ai/#__codelineno-6-3)```
import dspy
lm = dspy.LM("ollama_chat/llama3.2:1b", api_base="http://localhost:11434", api_key="")
dspy.configure(lm=lm)
```

First, install [SGLang](https://docs.sglang.ai/get_started/install.html) and launch its server with your LM.

```
> pip install "sglang[all]"
> pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ 

> CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct
```

If you don't have access from Meta to download `meta-llama/Llama-3.1-8B-Instruct`, use `Qwen/Qwen2.5-7B-Instruct` for example.

Next, connect to your local LM from your DSPy code as an `OpenAI`-compatible endpoint.

[1](https://dspy.ai/#__codelineno-8-1)
[2](https://dspy.ai/#__codelineno-8-2)
[3](https://dspy.ai/#__codelineno-8-3)
[4](https://dspy.ai/#__codelineno-8-4)```
lm = dspy.LM("openai/meta-llama/Llama-3.1-8B-Instruct",
             api_base="http://localhost:7501/v1",  # ensure this points to your port
             api_key="local", model_type="chat")
dspy.configure(lm=lm)
```

In DSPy, you can use any of the dozens of [LLM providers supported by LiteLLM](https://docs.litellm.ai/docs/providers). Simply follow their instructions for which `{PROVIDER}_API_KEY` to set and how to write pass the `{provider_name}/{model_name}` to the constructor.

Some examples:

*   `anyscale/mistralai/Mistral-7B-Instruct-v0.1`, with `ANYSCALE_API_KEY`
*   `together_ai/togethercomputer/llama-2-70b-chat`, with `TOGETHERAI_API_KEY`
*   `sagemaker/<your-endpoint-name>`, with `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION_NAME`
*   `azure/<your_deployment_name>`, with `AZURE_API_KEY`, `AZURE_API_BASE`, `AZURE_API_VERSION`, and the optional `AZURE_AD_TOKEN` and `AZURE_API_TYPE`

If your provider offers an OpenAI-compatible endpoint, just add an `openai/` prefix to your full model name.

[1](https://dspy.ai/#__codelineno-9-1)
[2](https://dspy.ai/#__codelineno-9-2)
[3](https://dspy.ai/#__codelineno-9-3)```
import dspy
lm = dspy.LM("openai/your-model-name", api_key="PROVIDER_API_KEY", api_base="YOUR_PROVIDER_URL")
dspy.configure(lm=lm)
```

Calling the LM directly.
Idiomatic DSPy involves using _modules_, which we define in the rest of this page. However, it's still easy to call the `lm` you configured above directly. This gives you a unified API and lets you benefit from utilities like automatic caching.

[1](https://dspy.ai/#__codelineno-10-1)
[2](https://dspy.ai/#__codelineno-10-2)```
lm("Say this is a test!", temperature=0.7)  # => ['This is a test!']
lm(messages=[{"role": "user", "content": "Say this is a test!"}])  # => ['This is a test!']
```

1) **Modules** help you describe AI behavior as _code_, not strings.[¶](https://dspy.ai/#1-modules-help-you-describe-ai-behavior-as-code-not-strings "Permanent link")
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

To build reliable AI systems, you must iterate fast. But maintaining prompts makes that hard: it forces you to tinker with strings or data _every time you change your LM, metrics, or pipeline_. Having built over a dozen best-in-class compound LM systems since 2020, we learned this the hard way—and so built DSPy to decouple AI system design from messy incidental choices about specific LMs or prompting strategies.

DSPy shifts your focus from tinkering with prompt strings to **programming with structured and declarative natural-language modules**. For every AI component in your system, you specify input/output behavior as a _signature_ and select a _module_ to assign a strategy for invoking your LM. DSPy expands your signatures into prompts and parses your typed outputs, so you can compose different modules together into ergonomic, portable, and optimizable AI systems.

Getting Started II: Build DSPy modules for various tasks

Try the examples below after configuring your `lm` above. Adjust the fields to explore what tasks your LM can do well out of the box. Each tab below sets up a DSPy module, like `dspy.Predict`, `dspy.ChainOfThought`, or `dspy.ReAct`, with a task-specific _signature_. For example, `question -> answer: float` tells the module to take a question and to produce a `float` answer.

Math RAG Classification Information Extraction Agents Multi-Stage Pipelines 

[1](https://dspy.ai/#__codelineno-11-1)
[2](https://dspy.ai/#__codelineno-11-2)```
math = dspy.ChainOfThought("question -> answer: float")
math(question="Two dice are tossed. What is the probability that the sum equals two?")
```

**Possible Output:**

```
Prediction(
    reasoning='When two dice are tossed, each die has 6 faces, resulting in a total of 6 x 6 = 36 possible outcomes. The sum of the numbers on the two dice equals two only when both dice show a 1. This is just one specific outcome: (1, 1). Therefore, there is only 1 favorable outcome. The probability of the sum being two is the number of favorable outcomes divided by the total number of possible outcomes, which is 1/36.',
    answer=0.0277776
)
```

[1](https://dspy.ai/#__codelineno-13-1)
[2](https://dspy.ai/#__codelineno-13-2)
[3](https://dspy.ai/#__codelineno-13-3)
[4](https://dspy.ai/#__codelineno-13-4)
[5](https://dspy.ai/#__codelineno-13-5)
[6](https://dspy.ai/#__codelineno-13-6)
[7](https://dspy.ai/#__codelineno-13-7)
[8](https://dspy.ai/#__codelineno-13-8)```
def search_wikipedia(query: str) -> list[str]:
    results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
    return [x["text"] for x in results]

rag = dspy.ChainOfThought("context, question -> response")

question = "What's the name of the castle that David Gregory inherited?"
rag(context=search_wikipedia(question), question=question)
```

**Possible Output:**

```
Prediction(
    reasoning='The context provides information about David Gregory, a Scottish physician and inventor. It specifically mentions that he inherited Kinnairdy Castle in 1664. This detail directly answers the question about the name of the castle that David Gregory inherited.',
    response='Kinnairdy Castle'
)
```

[1](https://dspy.ai/#__codelineno-15-1)
[2](https://dspy.ai/#__codelineno-15-2)
[3](https://dspy.ai/#__codelineno-15-3)
[4](https://dspy.ai/#__codelineno-15-4)
[5](https://dspy.ai/#__codelineno-15-5)
[6](https://dspy.ai/#__codelineno-15-6)
[7](https://dspy.ai/#__codelineno-15-7)
[8](https://dspy.ai/#__codelineno-15-8)
[9](https://dspy.ai/#__codelineno-15-9)
[10](https://dspy.ai/#__codelineno-15-10)
[11](https://dspy.ai/#__codelineno-15-11)```
from typing import Literal

class Classify(dspy.Signature):
    """Classify sentiment of a given sentence."""

    sentence: str = dspy.InputField()
    sentiment: Literal["positive", "negative", "neutral"] = dspy.OutputField()
    confidence: float = dspy.OutputField()

classify = dspy.Predict(Classify)
classify(sentence="This book was super fun to read, though not the last chapter.")
```

**Possible Output:**

```
Prediction(
    sentiment='positive',
    confidence=0.75
)
```

[1](https://dspy.ai/#__codelineno-17-1)
[2](https://dspy.ai/#__codelineno-17-2)
[3](https://dspy.ai/#__codelineno-17-3)
[4](https://dspy.ai/#__codelineno-17-4)
[5](https://dspy.ai/#__codelineno-17-5)
[6](https://dspy.ai/#__codelineno-17-6)
[7](https://dspy.ai/#__codelineno-17-7)
[8](https://dspy.ai/#__codelineno-17-8)
[9](https://dspy.ai/#__codelineno-17-9)
[10](https://dspy.ai/#__codelineno-17-10)
[11](https://dspy.ai/#__codelineno-17-11)
[12](https://dspy.ai/#__codelineno-17-12)
[13](https://dspy.ai/#__codelineno-17-13)
[14](https://dspy.ai/#__codelineno-17-14)
[15](https://dspy.ai/#__codelineno-17-15)
[16](https://dspy.ai/#__codelineno-17-16)
[17](https://dspy.ai/#__codelineno-17-17)```
class ExtractInfo(dspy.Signature):
    """Extract structured information from text."""

    text: str = dspy.InputField()
    title: str = dspy.OutputField()
    headings: list[str] = dspy.OutputField()
    entities: list[dict[str, str]] = dspy.OutputField(desc="a list of entities and their metadata")

module = dspy.Predict(ExtractInfo)

text = "Apple Inc. announced its latest iPhone 14 today." \
    "The CEO, Tim Cook, highlighted its new features in a press release."
response = module(text=text)

print(response.title)
print(response.headings)
print(response.entities)
```

**Possible Output:**

```
Apple Inc. Announces iPhone 14
['Introduction', "CEO's Statement", 'New Features']
[{'name': 'Apple Inc.', 'type': 'Organization'}, {'name': 'iPhone 14', 'type': 'Product'}, {'name': 'Tim Cook', 'type': 'Person'}]
```

[1](https://dspy.ai/#__codelineno-19-1)
[2](https://dspy.ai/#__codelineno-19-2)
[3](https://dspy.ai/#__codelineno-19-3)
[4](https://dspy.ai/#__codelineno-19-4)
[5](https://dspy.ai/#__codelineno-19-5)
[6](https://dspy.ai/#__codelineno-19-6)
[7](https://dspy.ai/#__codelineno-19-7)
[8](https://dspy.ai/#__codelineno-19-8)
[9](https://dspy.ai/#__codelineno-19-9)
[10](https://dspy.ai/#__codelineno-19-10)
[11](https://dspy.ai/#__codelineno-19-11)```
def evaluate_math(expression: str):
    return dspy.PythonInterpreter({}).execute(expression)

def search_wikipedia(query: str):
    results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
    return [x["text"] for x in results]

react = dspy.ReAct("question -> answer: float", tools=[evaluate_math, search_wikipedia])

pred = react(question="What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?")
print(pred.answer)
```

**Possible Output:**

```
5761.328
```

[1](https://dspy.ai/#__codelineno-21-1)
[2](https://dspy.ai/#__codelineno-21-2)
[3](https://dspy.ai/#__codelineno-21-3)
[4](https://dspy.ai/#__codelineno-21-4)
[5](https://dspy.ai/#__codelineno-21-5)
[6](https://dspy.ai/#__codelineno-21-6)
[7](https://dspy.ai/#__codelineno-21-7)
[8](https://dspy.ai/#__codelineno-21-8)
[9](https://dspy.ai/#__codelineno-21-9)
[10](https://dspy.ai/#__codelineno-21-10)
[11](https://dspy.ai/#__codelineno-21-11)
[12](https://dspy.ai/#__codelineno-21-12)
[13](https://dspy.ai/#__codelineno-21-13)
[14](https://dspy.ai/#__codelineno-21-14)
[15](https://dspy.ai/#__codelineno-21-15)
[16](https://dspy.ai/#__codelineno-21-16)
[17](https://dspy.ai/#__codelineno-21-17)
[18](https://dspy.ai/#__codelineno-21-18)
[19](https://dspy.ai/#__codelineno-21-19)
[20](https://dspy.ai/#__codelineno-21-20)
[21](https://dspy.ai/#__codelineno-21-21)
[22](https://dspy.ai/#__codelineno-21-22)
[23](https://dspy.ai/#__codelineno-21-23)
[24](https://dspy.ai/#__codelineno-21-24)
[25](https://dspy.ai/#__codelineno-21-25)
[26](https://dspy.ai/#__codelineno-21-26)
[27](https://dspy.ai/#__codelineno-21-27)
[28](https://dspy.ai/#__codelineno-21-28)
[29](https://dspy.ai/#__codelineno-21-29)
[30](https://dspy.ai/#__codelineno-21-30)
[31](https://dspy.ai/#__codelineno-21-31)
[32](https://dspy.ai/#__codelineno-21-32)```
class Outline(dspy.Signature):
    """Outline a thorough overview of a topic."""

    topic: str = dspy.InputField()
    title: str = dspy.OutputField()
    sections: list[str] = dspy.OutputField()
    section_subheadings: dict[str, list[str]] = dspy.OutputField(desc="mapping from section headings to subheadings")

class DraftSection(dspy.Signature):
    """Draft a top-level section of an article."""

    topic: str = dspy.InputField()
    section_heading: str = dspy.InputField()
    section_subheadings: list[str] = dspy.InputField()
    content: str = dspy.OutputField(desc="markdown-formatted section")

class DraftArticle(dspy.Module):
    def __init__(self):
        self.build_outline = dspy.ChainOfThought(Outline)
        self.draft_section = dspy.ChainOfThought(DraftSection)

    def forward(self, topic):
        outline = self.build_outline(topic=topic)
        sections = []
        for heading, subheadings in outline.section_subheadings.items():
            section, subheadings = f"## {heading}", [f"### {subheading}" for subheading in subheadings]
            section = self.draft_section(topic=outline.title, section_heading=section, section_subheadings=subheadings)
            sections.append(section.content)
        return dspy.Prediction(title=outline.title, sections=sections)

draft_article = DraftArticle()
article = draft_article(topic="World Cup 2002")
```

**Possible Output:**

A 1500-word article on the topic, e.g.

```
## Qualification Process

The qualification process for the 2002 FIFA World Cup involved a series of..... [shortened here for presentation].

### UEFA Qualifiers

The UEFA qualifiers involved 50 teams competing for 13..... [shortened here for presentation].

.... [rest of the article]
```

Note that DSPy makes it straightforward to optimize multi-stage modules like this. As long as you can evaluate the _final_ output of the system, every DSPy optimizer can tune all of the intermediate modules.

Using DSPy in practice: from quick scripting to building sophisticated systems.
Standard prompts conflate interface ("what should the LM do?") with implementation ("how do we tell it to do that?"). DSPy isolates the former as _signatures_ so we can infer the latter or learn it from data — in the context of a bigger program.

Even before you start using optimizers, DSPy's modules allow you to script effective LM systems as ergonomic, portable _code_. Across many tasks and LMs, we maintain _signature test suites_ that assess the reliability of the built-in DSPy adapters. Adapters are the components that map signatures to prompts prior to optimization. If you find a task where a simple prompt consistently outperforms idiomatic DSPy for your LM, consider that a bug and [file an issue](https://github.com/stanfordnlp/dspy/issues). We'll use this to improve the built-in adapters.

2) **Optimizers** tune the prompts and weights of your AI modules.[¶](https://dspy.ai/#2-optimizers-tune-the-prompts-and-weights-of-your-ai-modules "Permanent link")
---------------------------------------------------------------------------------------------------------------------------------------------------------------------

DSPy provides you with the tools to compile high-level code with natural language annotations into the low-level computations, prompts, or weight updates that align your LM with your program's structure and metrics. If you change your code or your metrics, you can simply re-compile accordingly.

Given a few tens or hundreds of representative _inputs_ of your task and a _metric_ that can measure the quality of your system's outputs, you can use a DSPy optimizer. Different optimizers in DSPy work by **synthesizing good few-shot examples** for every module, like `dspy.BootstrapRS`,[1](https://arxiv.org/abs/2310.03714)**proposing and intelligently exploring better natural-language instructions** for every prompt, like [`dspy.GEPA`](https://dspy.ai/tutorials/gepa_ai_program/)[2](https://arxiv.org/abs/2507.19457), `dspy.MIPROv2`,[3](https://arxiv.org/abs/2406.11695) and **building datasets for your modules and using them to finetune the LM weights** in your system, like `dspy.BootstrapFinetune`.[4](https://arxiv.org/abs/2407.10930) For detailed tutorials on running `dspy.GEPA`, please take a look at [dspy.GEPA tutorials](https://dspy.ai/tutorials/gepa_ai_program/).

Getting Started III: Optimizing the LM prompts or weights in DSPy programs

A typical simple optimization run costs on the order of $2 USD and takes around 20 minutes, but be careful when running optimizers with very large LMs or very large datasets. Optimization can cost as little as a few cents or up to tens of dollars, depending on your LM, dataset, and configuration.

Examples below rely on HuggingFace/datasets, you can install it by the command below.

```
> pip install -U datasets
```

Optimizing prompts for a ReAct agent Optimizing prompts for RAG Optimizing weights for Classification 

This is a minimal but fully runnable example of setting up a `dspy.ReAct` agent that answers questions via search from Wikipedia and then optimizing it using `dspy.MIPROv2` in the cheap `light` mode on 500 question-answer pairs sampled from the `HotPotQA` dataset.

[1](https://dspy.ai/#__codelineno-24-1)
[2](https://dspy.ai/#__codelineno-24-2)
[3](https://dspy.ai/#__codelineno-24-3)
[4](https://dspy.ai/#__codelineno-24-4)
[5](https://dspy.ai/#__codelineno-24-5)
[6](https://dspy.ai/#__codelineno-24-6)
[7](https://dspy.ai/#__codelineno-24-7)
[8](https://dspy.ai/#__codelineno-24-8)
[9](https://dspy.ai/#__codelineno-24-9)
[10](https://dspy.ai/#__codelineno-24-10)
[11](https://dspy.ai/#__codelineno-24-11)
[12](https://dspy.ai/#__codelineno-24-12)
[13](https://dspy.ai/#__codelineno-24-13)
[14](https://dspy.ai/#__codelineno-24-14)```
import dspy
from dspy.datasets import HotPotQA

dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

def search_wikipedia(query: str) -> list[str]:
    results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
    return [x["text"] for x in results]

trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train]
react = dspy.ReAct("question -> answer", tools=[search_wikipedia])

tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto="light", num_threads=24)
optimized_react = tp.compile(react, trainset=trainset)
```

An informal run like this raises ReAct's score from 24% to 51%, by teaching `gpt-4o-mini` more about the specifics of the task.

Given a retrieval index to `search`, your favorite `dspy.LM`, and a small `trainset` of questions and ground-truth responses, the following code snippet can optimize your RAG system with long outputs against the built-in `SemanticF1` metric, which is implemented as a DSPy module.

[1](https://dspy.ai/#__codelineno-25-1)
[2](https://dspy.ai/#__codelineno-25-2)
[3](https://dspy.ai/#__codelineno-25-3)
[4](https://dspy.ai/#__codelineno-25-4)
[5](https://dspy.ai/#__codelineno-25-5)
[6](https://dspy.ai/#__codelineno-25-6)
[7](https://dspy.ai/#__codelineno-25-7)
[8](https://dspy.ai/#__codelineno-25-8)
[9](https://dspy.ai/#__codelineno-25-9)
[10](https://dspy.ai/#__codelineno-25-10)
[11](https://dspy.ai/#__codelineno-25-11)```
class RAG(dspy.Module):
    def __init__(self, num_docs=5):
        self.num_docs = num_docs
        self.respond = dspy.ChainOfThought("context, question -> response")

    def forward(self, question):
        context = search(question, k=self.num_docs)   # defined in tutorial linked below
        return self.respond(context=context, question=question)

tp = dspy.MIPROv2(metric=dspy.evaluate.SemanticF1(decompositional=True), auto="medium", num_threads=24)
optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2)
```

For a complete RAG example that you can run, start this [tutorial](https://dspy.ai/tutorials/rag/). It improves the quality of a RAG system over a subset of StackExchange communities by 10% relative gain.

Click to show dataset setup code.

[1](https://dspy.ai/#__codelineno-26-1)
[2](https://dspy.ai/#__codelineno-26-2)
[3](https://dspy.ai/#__codelineno-26-3)
[4](https://dspy.ai/#__codelineno-26-4)
[5](https://dspy.ai/#__codelineno-26-5)
[6](https://dspy.ai/#__codelineno-26-6)
[7](https://dspy.ai/#__codelineno-26-7)
[8](https://dspy.ai/#__codelineno-26-8)
[9](https://dspy.ai/#__codelineno-26-9)
[10](https://dspy.ai/#__codelineno-26-10)
[11](https://dspy.ai/#__codelineno-26-11)
[12](https://dspy.ai/#__codelineno-26-12)
[13](https://dspy.ai/#__codelineno-26-13)
[14](https://dspy.ai/#__codelineno-26-14)
[15](https://dspy.ai/#__codelineno-26-15)
[16](https://dspy.ai/#__codelineno-26-16)
[17](https://dspy.ai/#__codelineno-26-17)
[18](https://dspy.ai/#__codelineno-26-18)```
import random
from typing import Literal

from datasets import load_dataset

import dspy
from dspy.datasets import DataLoader

# Load the Banking77 dataset.
CLASSES = load_dataset("PolyAI/banking77", split="train", trust_remote_code=True).features["label"].names
kwargs = {"fields": ("text", "label"), "input_keys": ("text",), "split": "train", "trust_remote_code": True}

# Load the first 2000 examples from the dataset, and assign a hint to each *training* example.
trainset = [
    dspy.Example(x, hint=CLASSES[x.label], label=CLASSES[x.label]).with_inputs("text", "hint")
    for x in DataLoader().from_huggingface(dataset_name="PolyAI/banking77", **kwargs)[:2000]
]
random.Random(0).shuffle(trainset)
```

[1](https://dspy.ai/#__codelineno-27-1)
[2](https://dspy.ai/#__codelineno-27-2)
[3](https://dspy.ai/#__codelineno-27-3)
[4](https://dspy.ai/#__codelineno-27-4)
[5](https://dspy.ai/#__codelineno-27-5)
[6](https://dspy.ai/#__codelineno-27-6)
[7](https://dspy.ai/#__codelineno-27-7)
[8](https://dspy.ai/#__codelineno-27-8)
[9](https://dspy.ai/#__codelineno-27-9)
[10](https://dspy.ai/#__codelineno-27-10)
[11](https://dspy.ai/#__codelineno-27-11)
[12](https://dspy.ai/#__codelineno-27-12)
[13](https://dspy.ai/#__codelineno-27-13)
[14](https://dspy.ai/#__codelineno-27-14)
[15](https://dspy.ai/#__codelineno-27-15)```
import dspy
lm=dspy.LM('openai/gpt-4o-mini-2024-07-18')

# Define the DSPy module for classification. It will use the hint at training time, if available.
signature = dspy.Signature("text, hint -> label").with_updated_fields("label", type_=Literal[tuple(CLASSES)])
classify = dspy.ChainOfThought(signature)
classify.set_lm(lm)

# Optimize via BootstrapFinetune.
optimizer = dspy.BootstrapFinetune(metric=(lambda x, y, trace=None: x.label == y.label), num_threads=24)
optimized = optimizer.compile(classify, trainset=trainset)

optimized(text="What does a pending cash withdrawal mean?")

# For a complete fine-tuning tutorial, see: https://dspy.ai/tutorials/classification_finetuning/
```

**Possible Output (from the last line):**

```
Prediction(
    reasoning='A pending cash withdrawal indicates that a request to withdraw cash has been initiated but has not yet been completed or processed. This status means that the transaction is still in progress and the funds have not yet been deducted from the account or made available to the user.',
    label='pending_cash_withdrawal'
)
```

An informal run similar to this on DSPy 2.5.29 raises GPT-4o-mini's score 66% to 87%.

What's an example of a DSPy optimizer? How do different optimizers work?
Take the `dspy.MIPROv2` optimizer as an example. First, MIPRO starts with the **bootstrapping stage**. It takes your program, which may be unoptimized at this point, and runs it many times across different inputs to collect traces of input/output behavior for each one of your modules. It filters these traces to keep only those that appear in trajectories scored highly by your metric. Second, MIPRO enters its **grounded proposal stage**. It previews your DSPy program's code, your data, and traces from running your program, and uses them to draft many potential instructions for every prompt in your program. Third, MIPRO launches the **discrete search stage**. It samples mini-batches from your training set, proposes a combination of instructions and traces to use for constructing every prompt in the pipeline, and evaluates the candidate program on the mini-batch. Using the resulting score, MIPRO updates a surrogate model that helps the proposals get better over time.

One thing that makes DSPy optimizers so powerful is that they can be composed. You can run `dspy.MIPROv2` and use the produced program as an input to `dspy.MIPROv2` again or, say, to `dspy.BootstrapFinetune` to get better results. This is partly the essence of `dspy.BetterTogether`. Alternatively, you can run the optimizer and then extract the top-5 candidate programs and build a `dspy.Ensemble` of them. This allows you to scale _inference-time compute_ (e.g., ensembles) as well as DSPy's unique _pre-inference time compute_ (i.e., optimization budget) in highly systematic ways.

3) **DSPy's Ecosystem** advances open-source AI research.[¶](https://dspy.ai/#3-dspys-ecosystem-advances-open-source-ai-research "Permanent link")
--------------------------------------------------------------------------------------------------------------------------------------------------

Compared to monolithic LMs, DSPy's modular paradigm enables a large community to improve the compositional architectures, inference-time strategies, and optimizers for LM programs in an open, distributed way. This gives DSPy users more control, helps them iterate much faster, and allows their programs to get better over time by applying the latest optimizers or modules.

The DSPy research effort started at Stanford NLP in Feb 2022, building on what we had learned from developing early [compound LM systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/) like [ColBERT-QA](https://arxiv.org/abs/2007.00814), [Baleen](https://arxiv.org/abs/2101.00436), and [Hindsight](https://arxiv.org/abs/2110.07752). The first version was released as [DSP](https://arxiv.org/abs/2212.14024) in Dec 2022 and evolved by Oct 2023 into [DSPy](https://arxiv.org/abs/2310.03714). Thanks to [250 contributors](https://github.com/stanfordnlp/dspy/graphs/contributors), DSPy has introduced hundreds of thousands of people to building and optimizing modular LM programs.

Since then, DSPy's community has produced a large body of work on optimizers, like [MIPROv2](https://arxiv.org/abs/2406.11695), [BetterTogether](https://arxiv.org/abs/2407.10930), and [LeReT](https://arxiv.org/abs/2410.23214), on program architectures, like [STORM](https://arxiv.org/abs/2402.14207), [IReRa](https://arxiv.org/abs/2401.12178), and [DSPy Assertions](https://arxiv.org/abs/2312.13382), and on successful applications to new problems, like [PAPILLON](https://arxiv.org/abs/2410.17127), [PATH](https://arxiv.org/abs/2406.11706), [WangLab@MEDIQA](https://arxiv.org/abs/2404.14544), [UMD's Prompting Case Study](https://arxiv.org/abs/2406.06608), and [Haize's Red-Teaming Program](https://blog.haizelabs.com/posts/dspy/), in addition to many open-source projects, production applications, and other [use cases](https://dspy.ai/community/use-cases/).

 Back to top [Next Learning DSPy](https://dspy.ai/learn/)

 © 2025 [DSPy](https://github.com/stanfordnlp)

 Made with [Material for MkDocs](https://squidfunk.github.io/mkdocs-material/)

[](https://github.com/stanfordnlp/dspy "github.com")[](https://discord.gg/XCGy2WDCQB "discord.gg")
