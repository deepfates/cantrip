---
title: "Julia Neagu on X: \"Why evals haven’t landed (yet) + lessons from building them for Copilot\" / X"
url: "https://x.com/julianeagu/status/1964704824299253888?s=09&t=A3yGaeNJQYUXMoSGx7ls_Q&rw_tt_thread=True"
date_fetched: "2026-02-16"
type: twitter
---

While I’ve been part of the evals conversation online, I wanted to step back, explain why this debate has felt so fractured, and share a few lessons from building evals at GitHub Copilot and later at Quotient.

LLMs showed up as APIs. That was familiar ground for engineers, and they were the ones tasked to roll them out into products. The AI tools that stuck all had one thing in common: good ergonomics for engineers. They felt like extensions or analogies of tools developers already used every day:

*   databases (vector DBs, log + trace storage)

*   other APIs (search, computer use, etc. — plug-and-play)

*   monitoring (still emerging)

The developer / engineer preference matrix explains why, two years into “evals matter” discourse, eval tooling still hasn’t broken through: the ergonomics don’t map to known developer workflows.

> I feel like we need to bring data science back into AI Eng because its absence is really showing in the discourse. —

This is the right take. One of the reasons evals have felt so painful is because we’ve been squeezing two roles into one: data scientist + software engineer.

Gathering and analyzing data has traditionally been a data science job. Engineers don’t live there. Their interactions with product data are exactly three things:

1.   (unit) tests

2.   CI/CD

3.   monitoring + alerts

These are the surface areas engineers are used to operating with. Which is why “just look at your data” doesn’t land in practice. Most people shipping AI right now are engineers, not data scientists. Engineers are not used to operating in notebooks and spreadsheets, and they will avoid it at all costs.

Platforms focused on manual evals and “look at your data” workflows are naturally targeted at data scientists and the machine learning crowd. Almost all evaluation platforms out there are not targeted at the engineers turning LLM APIs into products. Their ergonomics will have a hard time getting real traction with engineers. I’m not saying it’ll never happen, but after two years of evangelism in this area the gap is still obvious.

As a side note, I have a ton of respect for the content that

,

,

, and others have put out in the past couple of years. A lot of them are friends and former colleagues who provide really smart and sound advice, which I wish was adopted by more people in the space. In this blog, I am mostly focused on what my impression is of the status quo in practice in the industry, and less so on what I think the status quo should be, though I do touch upon this towards the end.

Is pizza a pie? Is a taco a sandwich? Are A/B tests and evals different? Are evals tests? Are they dead? Are they alive? We can all have fun debating semantics, but let's be wary of noise and distractions that don’t really solve problems. They’re fun for half an hour over lunch, but if they hijack your entire week maybe less so.

I stand by

’s definition:

> What do I mean by evals?Hamel and I define it simply as the systematic measurement of application quality.Notice that this doesn’t imply any particular metric or method, nor does it have to be a single number. It doesn’t have to be perfectly accurate either—it can be an estimate. The point is that it is systematic rather than ad hoc: you’re checking quality in a continuous, deliberate way.

Which to me equates to: “Do you know anything about how your product works at any given time? Cool, you’re doing evals.” The only thing that ultimately matters is whether your product works or not.

Two years ago, I ran the GitHub team building evals for Copilot. Mostly data scientists, some engineers.

,

, and other really cool people, very few of whom are still there today.

It was 2022/2023 and most of us had little idea what we were doing; Copilot was just an OAI wrapper, but it was taking off like crazy and had turned into one of GitHub’s main revenue sources. 100M developers were using it. To my knowledge, we were the only team shipping that type of AI product to that amount of users. There was a ton of pressure to make it good.

Our evaluation stack was:

*   a big eval harness for benchmarking changes on coding data (mostly regression tests to make sure nothing broke)

*   A/B tests and monitoring on 

The harness used code from publicly available repositories; we would focus on generating completions of code and ensuring tests passed. The reason the harness worked well is because code is objectively testable.

The harness was useful, but in reality the pressure to move fast and ship was so high that if something passed A/B, we shipped it.

The real bottleneck was Shiproom. Every week, we had a two-hour meeting driven by execs from GitHub and Microsoft. Researchers from both companies would present A/B test results on model and prompt swaps, one by one. Shiproom not just slowed things down, but it also made us more focused on incremental improvements.

At the core of the A/B experiments were metrics layered on top of Copilot telemetry. This was also messy, because some of the telemetry came from the VSCode teams inside Microsoft. We were focused on improving this funnel:

[![Image 1: Image](https://pbs.twimg.com/media/G0QE4GZWoAATjOM?format=png&name=small)](https://x.com/julianeagu/article/1964704824299253888/media/1964700699276517376)

From: How to Evaluate LLMs: A Complete Metric Framework.

And measured a set of metrics that included:

*   Average edit distance between prompts (i.e. measuring reprompting)

*   Average edit distance between LLM response and retained content (i.e. measuring suggestion acceptance and retention)

We deliberately focused on metrics users provided implicitly (e.g. accepting a completion) rather than explicit signals like thumbs up/down. Implicit signals avoided bias, self-selection, and inconsistency, while better reflecting how people were actually using the product. I’ve written more about this distinction

.

Our flagship metric was the

, which at that time hovered around 30%.

[![Image 2: Image](https://pbs.twimg.com/media/G0QFR_iW4AAKszo?format=jpg&name=small)](https://x.com/julianeagu/article/1964704824299253888/media/1964701144111833088)

From: Sea Change in Software Development: Economic and Productivity Analysis of the AI-Powered Developer Lifecycle.

This stack was, for the most part, something built by data scientists for data scientists. When Copilot started expanding to more teams in mid-2023 (e.g. Copilot Chat), testing early versions was significantly more vibes-based. That shift was driven both by the pressure to ship and by the skill sets of the teams involved. It mirrored what we later found across most of the industry.

When

and I started Quotient in 2023, we carried that data-science-first bias with us. First versions of the platform were all about “offline evals”: throw your prompt/model combos at datasets, get back metrics like similarity or faithfulness.

When we talked to customers, we hit the same walls every time:

1.   teams vibe-shipping or running tiny evals manually

2.   general aversion towards building comprehensive evaluation datasets

3.   the ones who cared were already investing heavily in manual checks and in-house tooling

tl;dr: vibe-shipping was easy, dataset building was painful, and in-house tools were custom-fit.

We actually barked up this tree for quite a bit, thinking about ways to reduce the barrier for running offline evals as much as possible: anything from automatic synthetic data pipelines to straight-up building the datasets ourselves for our customers.

In the end, there was no way to bootstrap an evaluation platform with offline evals alone.

Luckily, we came out of this with a few key insights:

*   Despite claims to the contrary, for the most part AI products are still shipped on vibes. This was surprising to us, especially given how loud the online discourse was around evals.

pretty much hit the nail on the head with

. There was, and still is, a significant amount of

. 
*   Unlike our team at GitHub, most of AI teams were made up of engineers, not data scientists or researchers. If evals did not work for them for them, they wouldn’t be used.

We ended up throwing out most of our product and focusing on building the best match between what we consider necessary for shipping and improving AI agents, and what developers would be willing to adopt.

*   Do online evaluation first. Most of our users were “testing in prod,” so we followed them there with few-lines-of-code tracing and logging. This was the path of least resistance.

*   Automate as much of possible. Agent trace complexity goes up exponentially with agent complexity; they are fairly illegible data structures and manual trace review is not a sustainable strategy for uncovering errors and root causes. We made parsing and analyzing them our expertise.

*   Provide critical out-of-the-box metrics. Most developers did not want to spend time crafting LLM judge prompts or manually labeling data. We packaged our own expertise into specialized detector for critical issues like

and

. 
*   Provide visibility. Most commonly, developers don’t know what issues their users run into with their AI products. We showed them the biggest problem areas (lowest metrics, most traffic).

*   Help customers discover their own flagship metrics. The hardest part of building evals isn’t the metrics, it’s agreeing on what ‘good’ even means for a product. Sometimes that’s objective (does this code compile), other times it’s subjective (do users accept the suggestion w/o editing). We always strongly recommend to our customers that they have a strong, quantifiable definition of “good” for their products. The most successful always do.

Despite the progress, I actually don’t think that what we have today is enough. Our goal is to automatically improve AI agents over time. Then, and only then, will evals become something that is bought, not sold.

If I were starting out building an AI product today, here’s what I would do:

1.   Vibes are the first evals. Ship something fast. See what works, what doesn’t, and whether anyone cares. If the vibes are good, keep going.

2.   Before you ship, add some monitoring. The entire point of shipping is to run a test. It doesn’t have to be rigorous, but at the very least: save all the data.

3.   Chase your users, but save the data. If users love it, keep building. Many successful founders told me their evals were just users in Slack/Discord yelling about the product. That’s valid signal. Chase it.

4.   Fix your stuff. You will ship misses. That’s fine. The monitoring you added earlier will help you see how bad they are and why. For example, at Quotient, we provide reasoning for all our

to facilitate root cause analysis and

to surface the biggest issues. 
5.   When success comes, invest in reliability. If you do steps 1-4 well, you will end up with a fairly legit AI product that people care about. It is now time to spend some engineering cycles on reliability: - Add some CI/CD. Pick something lightweight and easy to use. For example, developers have incorporated our OSS

library into their CI/CD. - Track your flagship metrics. At Copilot, we kept an eye on completion acceptance rate. ours will be different. Ideally, this will be measured automatically as you monitor (see

as example). 

For what it’s worth, I have yet to see an example where a company in the past two years has slowed down to focus on evals and systematic, step-wise improvement and has actually beat their competition. Pretty much everyone I know who is “winning” is shipping at a crazy pace and figuring stuff out as they go. They have something in place for not shipping slop (or, at least, unshipping slop when it happens), but the dominant strategy is still ship fast and check later. I will leave you with one final thought:

There’s a lot of advice out there. The best advice comes from people actually building products. There’s are some nice write-ups out there from companies that are pretty far ahead in their AI journey, sharing how they evaluate their system (e.g

). They provide a (crafted, partial) view into what it takes to ship a good product with traction. I guarantee you it’s not the full story. But it’s closer to the full story because it’s coming from someone who’s building an AI product themselves.

Good luck!
