---
title: "Robot Face: Learning Loops"
url: https://www.deepfates.com/robot-face-learning-loops
date_fetched: 2026-02-16
author: deepfates
---

# Learning Loops

_Originally published on [Robot Face](https://robotface.substack.com/p/learning-loops)._

## Previously

The author previously discussed the learning curve used in artificial intelligence training, explaining how cyclical learning rates alternate between exploration and exploitation. They emphasize that training time is another critical hyperparameter -- shorter training periods enable more frequent experimentation with models.

## How long does it take to train a neural net?

Training duration varies dramatically. The author has trained image classifiers in under a minute, language models in 8-10 hours, and art-generating models over multiple days.

The timeframe influences creative engagement: "if it takes less than five minutes to train or generate, I will sit and play with it for hours." Longer training cycles create different work patterns, while multi-day runs significantly reduce experimentation enthusiasm.

Training can be accelerated by reducing neural network complexity or training data volume, though this typically sacrifices performance. However, techniques like progressive resizing prove both faster and higher-quality than traditional approaches.

## Applying this to human learning

Reviewing previous thoughts at regular intervals mimics how neural networks test accuracy between epochs. Social media platforms offer limited retrospective features focused on entertainment rather than insight.

The author uses Thread Helper, a browser extension that replaces distracting content panels with dynamically-updating lists of past tweets while composing. This surfaces "thoughts from long ago for me to thread together and re-interpret in the moment," transforming Twitter into a thinking tool.

Thread Helper functions as a digital garden, "hydrating your memory" by directing attention backward. Like the Drawer from design fiction, it brings information directly to users, reducing the feedback loop and accelerating learning through artificial recall mechanisms.
