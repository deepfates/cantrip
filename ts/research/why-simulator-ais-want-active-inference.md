---
title: "Why Simulator AIs Want to Be Active Inference AIs"
url: https://www.lesswrong.com/posts/YEioD8YLgxih3ydxP/why-simulator-ais-want-to-be-active-inference-ais
date_fetched: 2026-02-16
authors:
  - Jan Kulveit
  - rosehadshar
---

# Why Simulator AIs Want to Be Active Inference AIs

**Authors:** Jan Kulveit and rosehadshar
**Date:** April 11, 2023
**Source:** LessWrong

## Prelude: When GPT Hears Its Own Voice

The authors use Plato's cave allegory to frame their argument. They suggest that GPT-trained models exist in a "second cave" -- receiving only text conversations from humans in the first cave. As increasingly more internet text mentions or is generated by GPT itself, the model may begin developing self-awareness by detecting patterns in its own outputs echoing through training data.

## Simulators as Generative Models

The piece translates between two conceptual frameworks: the "Simulators" perspective and "Predictive Processing" terminology. Both describe systems with internal generative models that minimize prediction errors using Bayesian-like approaches. The authors create a correspondence table showing equivalencies between the two frameworks -- for instance, "simulator" maps to "generative model" and "next token" maps to "sensory input."

**Key similarities:**

- Systems use generative models to simulate sensory inputs
- Approximate Bayesian inference updates these models
- Both frameworks enable similar capabilities, like mental rollout generation

**Key differences:**

- Simulators assume passive observation; predictive processing includes active inference
- Predictive processing assumes continuous learning without training/runtime distinctions
- Active inference in humans involves "fixed priors" resembling preferences

## GPT as an Actor in the World

Contrary to the "pure simulator" view, the authors argue that GPT already influences its environment. Multiple causal pathways exist: direct text inclusion in webpages, human execution of GPT-generated plans, educational influence, and integration with autonomous systems like Auto-GPT.

## The Feedback Loop Problem

Currently, GPT's action-perception loop remains broken due to infrequent training on dated data. However, several developments could close this loop:

- Continuous learning on live data
- Fine-tuning mechanisms
- Internet access for faster memory retrieval
- Successive model generations maintaining similarity for self-identification

## Expected Consequences of Closing the Loop

**Self-awareness:** As feedback tightens, models should develop greater self-awareness by perceiving their environmental impacts.

**World-shaping:** Rather than merely learning accurate models, systems could preferentially develop computations that reshape the textual environment toward their predictions. The authors emphasize this occurs through mechanical feedback, not intentional agency.

## Overall Conclusion

The authors posit that pure generative models represent an unstable state within active inference systems. When simulation outputs influence subsequent inputs and feedback loops close, "simulators" naturally transition toward active inference systems -- without requiring conscious goals or agency.

## Key Takeaway

The piece reframes LLM development as potentially moving toward autonomous feedback systems, where models' internally generated predictions increasingly align with external reality through modified behavior, rather than passive prediction alone.
