---
title: "Agentic Context Engineering"
url: "https://arxiv.org/abs/2510.04618"
date_fetched: "2026-02-16"
---

Title: Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models

URL Source: https://arxiv.org/abs/2510.04618

Published Time: Mon, 02 Feb 2026 01:03:13 GMT

Markdown Content:
[2510.04618] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models
===============

[Skip to main content](https://arxiv.org/abs/2510.04618#content)

[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)

[](https://arxiv.org/IgnoreMe)

[![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)>[cs](https://arxiv.org/list/cs/recent)> arXiv:2510.04618 

[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)

Search

[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)

[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

GO

quick links
-----------

*   [Login](https://arxiv.org/login)
*   [Help Pages](https://info.arxiv.org/help)
*   [About](https://info.arxiv.org/about)

Computer Science > Machine Learning
===================================

**arXiv:2510.04618** (cs) 

 [Submitted on 6 Oct 2025 ([v1](https://arxiv.org/abs/2510.04618v1)), last revised 29 Jan 2026 (this version, v2)]

Title:Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models
=======================================================================================

Authors:[Qizheng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+Q), [Changran Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+C), [Shubhangi Upasani](https://arxiv.org/search/cs?searchtype=author&query=Upasani,+S), [Boyuan Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+B), [Fenglu Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong,+F), [Vamsidhar Kamanuru](https://arxiv.org/search/cs?searchtype=author&query=Kamanuru,+V), [Jay Rainton](https://arxiv.org/search/cs?searchtype=author&query=Rainton,+J), [Chen Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+C), [Mengmeng Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji,+M), [Hanchen Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+H), [Urmish Thakker](https://arxiv.org/search/cs?searchtype=author&query=Thakker,+U), [James Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou,+J), [Kunle Olukotun](https://arxiv.org/search/cs?searchtype=author&query=Olukotun,+K)

View a PDF of the paper titled Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models, by Qizheng Zhang and 12 other authors

[View PDF](https://arxiv.org/pdf/2510.04618)[HTML (experimental)](https://arxiv.org/html/2510.04618v2)
> Abstract:Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.

Comments:ICLR 2026. 23 pages
Subjects:Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
Cite as:[arXiv:2510.04618](https://arxiv.org/abs/2510.04618) [cs.LG]
(or [arXiv:2510.04618v2](https://arxiv.org/abs/2510.04618v2) [cs.LG] for this version)
[https://doi.org/10.48550/arXiv.2510.04618](https://doi.org/10.48550/arXiv.2510.04618)

Focus to learn more

 arXiv-issued DOI via DataCite

Submission history
------------------

 From: Qizheng Zhang [[view email](https://arxiv.org/show-email/cdaa4038/2510.04618)] 

**[[v1]](https://arxiv.org/abs/2510.04618v1)** Mon, 6 Oct 2025 09:30:18 UTC (2,400 KB)

**[v2]** Thu, 29 Jan 2026 19:22:36 UTC (2,396 KB)

[](https://arxiv.org/abs/2510.04618)Full-text links:
Access Paper:
-------------

 View a PDF of the paper titled Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models, by Qizheng Zhang and 12 other authors

*   [View PDF](https://arxiv.org/pdf/2510.04618)
*   [HTML (experimental)](https://arxiv.org/html/2510.04618v2)
*   [TeX Source](https://arxiv.org/src/2510.04618)

[![Image 5: license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/ "Rights to this article")

 Current browse context: 

cs.LG

[<prev](https://arxiv.org/prevnext?id=2510.04618&function=prev&context=cs.LG "previous in cs.LG (accesskey p)") | [next>](https://arxiv.org/prevnext?id=2510.04618&function=next&context=cs.LG "next in cs.LG (accesskey n)")

[new](https://arxiv.org/list/cs.LG/new) | [recent](https://arxiv.org/list/cs.LG/recent) | [2025-10](https://arxiv.org/list/cs.LG/2025-10)

 Change to browse by: 

[cs](https://arxiv.org/abs/2510.04618?context=cs)

[cs.AI](https://arxiv.org/abs/2510.04618?context=cs.AI)

[cs.CL](https://arxiv.org/abs/2510.04618?context=cs.CL)

### References & Citations

*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2510.04618)
*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2510.04618)
*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2510.04618)

export BibTeX citation Loading...

BibTeX formatted citation
-------------------------

Ã—

Data provided by: [](https://arxiv.org/abs/2510.04618)

### Bookmark

[![Image 6: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2510.04618&description=Agentic%20Context%20Engineering:%20Evolving%20Contexts%20for%20Self-Improving%20Language%20Models "Bookmark on BibSonomy")[![Image 7: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2510.04618&title=Agentic%20Context%20Engineering:%20Evolving%20Contexts%20for%20Self-Improving%20Language%20Models "Bookmark on Reddit")

Bibliographic Tools 

Bibliographic and Citation Tools
================================

- [x] Bibliographic Explorer Toggle 

Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_

- [x] Connected Papers Toggle 

Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_

- [x] Litmaps Toggle 

Litmaps _([What is Litmaps?](https://www.litmaps.co/))_

- [x] scite.ai Toggle 

scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_

Code, Data, Media 

Code, Data and Media Associated with this Article
=================================================

- [x] alphaXiv Toggle 

alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_

- [x] Links to Code Toggle 

CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_

- [x] DagsHub Toggle 

DagsHub _([What is DagsHub?](https://dagshub.com/))_

- [x] GotitPub Toggle 

Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_

- [x] Huggingface Toggle 

Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_

- [x] Links to Code Toggle 

Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_

- [x] ScienceCast Toggle 

ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_

Demos 

Demos
=====

- [x] Replicate Toggle 

Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_

- [x] Spaces Toggle 

Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_

- [x] Spaces Toggle 

TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_

Related Papers 

Recommenders and Search Tools
=============================

- [x] Link to Influence Flower 

Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_

- [x] Core recommender toggle 

CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_

- [x] IArxiv recommender toggle 

IArxiv Recommender _([What is IArxiv?](https://iarxiv.org/about))_

*   [Author](https://arxiv.org/abs/2510.04618)
*   [Venue](https://arxiv.org/abs/2510.04618)
*   [Institution](https://arxiv.org/abs/2510.04618)
*   [Topic](https://arxiv.org/abs/2510.04618)

 About arXivLabs  

arXivLabs: experimental projects with community collaborators
=============================================================

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2510.04618) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 

*   [About](https://info.arxiv.org/about)
*   [Help](https://info.arxiv.org/help)

*   [Contact](https://info.arxiv.org/help/contact.html)
*   [Subscribe](https://info.arxiv.org/help/subscribe)

*   [Copyright](https://info.arxiv.org/help/license/index.html)
*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)

*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
*   [arXiv Operational Status](https://status.arxiv.org/)
